# Tools

<aside>
ğŸ’¡ Google BigQuery and Snowflake

</aside>

# TPC-H Data generate

- Data generating: [Manual_1_local](https://tedamoh.com/en/blog/55-data-modeling/78-generating-large-example-data-with-tpc-h)
- ForÂ `XXX.tbl`Â files, however, there is a bug inÂ `dbgen`Â which generates an extraÂ `|`Â at the end of each line. To fix it, run the following command:
- $ `for i in `ls *.tbl`; do sed 's/|$//' $i > ${i/tbl/csv}; echo $i; done;`

# Google BigQuery

<aside>
ğŸ’¡ ****æ–°æœåŠ¡è´¦å· HMAC å¯†é’¥****

hmac1-455@advanceddb-405622.iam.gserviceaccount.com

è®¿é—®å¯†é’¥GOOG1E6FKGZ5WLHO3NNSF77PBZCYPZPK2VED3LYQH3XIOL2YVJZPRNLBA3JDK

å¯†é’¥
32A8aEifoLYJcF1XTofOhx5zcXEfVeJ0aFrPwtaA

</aside>

## Create Table

- å°†csvæ–‡ä»¶çš„æœ€åä¸€åˆ—ç©ºåˆ—åˆ é™¤â€”ã€‹remove_null_col.py
- å°†æ–‡ä»¶ä¸Šä¼ cloud storage
- åœ¨sql consoleä¸­åˆ›å»ºè¡¨æ ¼â€”ã€‹big queryä¸­ä¸æ”¯æŒprimary keyï¼Œç»“æ„ç±»å‹çš„å®šä¹‰ä¸snowflakeä¹Ÿä¸åŒï¼Œè¿›è¡Œä¿®æ”¹
- åœ¨cloudä¸­ç”¨bpå‘½ä»¤ï¼š
ziyongzhang01@cloudshell:~ (advanceddb-405622)$ bq load --noreplace --source_format=CSV PUBLIC.REGION gs://bucket_bdma_adb/TPC-H_05/region.csv

åœ¨databaseä¸­åˆ›å»ºè¡¨æ ¼ï¼š
$ ./create_table.sh TPCH_1G

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸€æ¬¡æ€§å†™å…¥æ•°æ®ï¼š
$ ./load_data.sh your_database_name
$ ./load_data.sh <your_database> <bucket_folder> TPC-H_05
ä»¥ä¸‹å‘½ä»¤ä¿®æ”¹shæ–‡ä»¶ï¼š(ctrl+x ä¿å­˜ï¼Œenter é€€å‡º)
$ nano load_data.sh
ä»¥ä¸‹å‘½ä»¤è®©shæ–‡ä»¶å¯æ‰§è¡Œï¼š
$ chmod +x load_data.sh
- DATABASES:
PUBLIC
TPCH_1G
TPCH_2G
TPCH_3G
TPCH_5G

```bash
#!/bin/bash 
# å°†æ•°æ®ä»cloud storageä¸­å†™å…¥è¡¨æ ¼
# æ£€æŸ¥æ˜¯å¦æä¾›äº†æ•°æ®åº“åç§°
if [ -z "$1" ]; then
  echo "Usage: $0 <database_name>"
  exit 1
fi

project_id="advanceddb-405622"
database_name="$1"
datasets=("CUSTOMER" "LINEITEM" "NATION" "ORDERS" "PART" "PARTSUPP" "REGION" "SUPPLIER")

for dataset in "${datasets[@]}"
do
  lowercase_dataset=$(echo "$dataset" | tr '[:upper:]' '[:lower:]')
  bq load --noreplace --source_format=CSV ${project_id}:${database_name}.${dataset} gs://bucket_bdma_adb/TPC-H_05/${lowercase_dataset}.csv
done
```

```bash
#!/bin/bash
# chmod +x run_script.sh 
# ./run_script.sh your-dataset-name

# é¡¹ç›® ID
project_id="your-project-id"
# é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ä¼ é€’çš„æ•°æ®é›†åç§°
dataset_name="$1"
# Cloud Storage ä¸­ SQL æ–‡ä»¶çš„è·¯å¾„
sql_folder="gs://bucket_bdma_adb/queries_bq_general"
# ç»“æœè¾“å‡º CSV æ–‡ä»¶è·¯å¾„
output_csv="output_runtimes_${dataset_name}.csv"
# æ£€æŸ¥æ˜¯å¦æä¾›äº†æ•°æ®é›†åç§°ä½œä¸ºå‚æ•°
if [ -z "$dataset_name" ]; then
  echo "Usage: $0 <dataset_name>"
  exit 1
fi
# è·å–æ‰€æœ‰ SQL æ–‡ä»¶åˆ—è¡¨
sql_files=$(gsutil ls ${sql_folder}/*.sql)
# åˆ›å»ºè¾“å‡º CSV æ–‡ä»¶å¹¶å†™å…¥æ ‡é¢˜
echo "SQL_File,Run_Time" > ${output_csv}

# å¾ªç¯éå†å¹¶æ‰§è¡Œæ¯ä¸ª SQL æ–‡ä»¶
for sql_file in ${sql_files[@]}
do
  # æå–è¡¨åï¼ˆå‡è®¾æ–‡ä»¶åä¸º table_name.sqlï¼‰
  table_name=$(basename -- "${sql_file}")
  table_name="${table_name%.sql}"

  # è®°å½• SQL æ–‡ä»¶åå’Œæ‰§è¡Œæ—¶é—´çš„èµ·å§‹æ—¶é—´
  start_time=$(date +"%s")
  # æ‰§è¡Œ SQL æ–‡ä»¶
  bq query --use_legacy_sql=false --project_id=${project_id} --dataset_id=${dataset_name} < ${sql_file}
  # è®°å½•è¿è¡Œæ—¶é—´
  end_time=$(date +"%s")
  run_time=$((end_time - start_time))
  # è¾“å‡ºç»“æœåˆ° CSV æ–‡ä»¶
  echo "${table_name},${run_time}" >> ${output_csv}
done
```

## SQL è¯­å¥ä¿®æ”¹

```
`advanceddb-405622.PUBLIC.PART`,
	`advanceddb-405622.PUBLIC.SUPPLIER`,
	`advanceddb-405622.PUBLIC.PARTSUPP`,
	`advanceddb-405622.PUBLIC.NATION`,
	`advanceddb-405622.PUBLIC.REGION`,
	`advanceddb-405622.PUBLIC.CUSTOMER`,
	`advanceddb-405622.PUBLIC.LINEITEM`,
	`advanceddb-405622.PUBLIC.ORDERS`

`${PROJECT_ID}.${DATASET}.PART`,
	`${PROJECT_ID}.${DATASET}.SUPPLIER`,
	`${PROJECT_ID}.${DATASET}.PARTSUPP`,
	`${PROJECT_ID}.${DATASET}.NATION`,
	`${PROJECT_ID}.${DATASET}.REGION`,
	`${PROJECT_ID}.${DATASET}.CUSTOMER`,
	`${PROJECT_ID}.${DATASET}.LINEITEM`,
	`${PROJECT_ID}.${DATASET}.ORDERS`

```

```sql
-- 
-- 1
where
	l_shipdate <= DATEADD(DAY, 3, DATE '1998-12-01')
-- æ”¹ä¸º
WHERE
    DATE(l_shipdate) <= DATE_ADD(DATE '1998-12-01', INTERVAL 3 DAY)

where
  DATE(PARSE_DATE('%m/%d/%Y',  l_shipdate)) <= DATE_ADD(DATE '1998-12-01', INTERVAL 3 DAY)

-- 13
-- The error you're encountering is due to the use of subqueries in the FROM clause, which is not supported in BigQuery. 
-- Instead, you can use a common table expression (CTE) to achieve the same result. Here's the modified query:
```

## æ‰§è¡ŒSQLè¯­å¥

- run_main_script.pyæ–‡ä»¶ï¼Œåœ¨terminalä¸­è¿è¡Œï¼Œå‚æ•°ä¸ºdataset_name.

# SNOWFLAKE

## Introduction

- Introduction reference: ä»‹ç»çš„æ–‡ç« ä¸ç¿»è¯‘åšå®¢ [paper](https://fuzhe1989.github.io/2020/12/28/the-snowflake-elastic-data-warehouse/)
- Problem: You can load data from individual files [up to 50 MB](https://docs.snowflake.com/en/user-guide/data-load-web-ui) in size.

## æ•°æ®ç”Ÿæˆ

- terminal: dgen å‘½ä»¤ç”Ÿæˆtblæ–‡ä»¶
- python: ç”¨tbl_to_csvè½¬æ¢æˆcsvæ–‡ä»¶
- terminal: ç”¨putå‘½ä»¤å°†æ–‡ä»¶ä¸Šä¼ åˆ°snowflake
- python: å»ºç«‹stageï¼Œåœ¨schemaä¸­åˆ›å»ºç©ºè¡¨
- å°†ä¸Šä¼ çš„æ–‡ä»¶å†™å…¥ç©ºè¡¨ä¸­

## Upload data from local files

- - åˆ›å»ºå¤–éƒ¨å­˜å‚¨åŒºï¼š[ç½‘ç«™](https://docs.snowflake.com/en/sql-reference/sql/create-stage)
CREATE STAGE your_external_stage;
- - ä¸Šä¼ æ•°æ®æ–‡ä»¶åˆ°å¤–éƒ¨å­˜å‚¨åŒº
PUT file:///path/to/generated/data/*.tbl @your_external_stage;

## Python è‡ªåŠ¨æ‰§è¡Œæ–‡ä»¶

- ä¸‹è½½[snowflake connector for python](https://docs.snowflake.com/developer-guide/python-connector/python-connector)
- To install the latest Python Connector for Snowflake, use:
    - $ pip install snowflake-connector-python
- After configuring your driver, you can evaluate and troubleshoot your network connectivity to Snowflake usingÂ [SnowCD](https://docs.snowflake.com/en/user-guide/snowcd).
- Import theÂ `snowflake.connector`Â module, execute the following command:
    - import snowflake.connector
- Connect using the default authenticator
    - account url: [https://wv59487.europe-west4.gcp.snowflakecomputing.com](https://wv59487.europe-west4.gcp.snowflakecomputing.com/)
	account identifier: wv59487.europe-west4.gcp
        
        organization: **TTUZQMN**
        
        account: **AE33915**
        
        locator: **WV59487**
        
        username: **ZIYONGZHANG**
        
        password: **Snowflake01#**
        
        warehouse: **COMPUTE_WH**
        
        database: 
        **SNOWFLAKE_SAMPLE_DATA
        ADB**
        
        schema: **PUBLIC , TPCH_1 , TPCH_2 , TPCH_3**
        
        conn = snowflake.connector.connect(
        user=USER,
        password=PASSWORD,
        account=ACCOUNT_url,
        warehouse=WAREHOUSE,
        database=DATABASE,
        schema=SCHEMA
        )
        
        -a wv59487.europe-west4.gcp -u **ZIYONGZHANG -d ADB -s PUBLIC** 
        
    - sql_directory: /Users/zzy13/Desktop/Classes_at_ULB/Advanced_Databases_415/Project/TPC-H_V3.0.1/dbgen/queries_test
    - Create table:
    /Users/zzy13/Desktop/Classes_at_ULB/Advanced_Databases_415/Project/TPC-H_V3_0_1/scripts/Create_table/create_table.sql
    - Terminal:
    $ snowsql -a wv59487.europe-west4.gcp -u ZIYONGZHANG -d ADB -s PUBLIC
    $

## SQL ä¿®æ”¹

- ä¿®æ”¹è¯­å¥
    
    SQL_1
    where
    l_shipdate <= DATEADD(DAY, 3, DATE '1998-12-01')
    
    SQL_2
    and p_size = 8
    and p_type like '%BRASS'
    and r_name = 'EUROPEâ€™
    
    SQL_3
    c_mktsegment = 'BUILDING'
    and c_custkey = o_custkey
    and l_orderkey = o_orderkey
    and o_orderdate < date '1994-06-13'
    and l_shipdate > date '1994-09-30'
    
    SQL_4
    where
    o_orderdate >= date '1997-11-11'
    and o_orderdate < DATEADD(MONTH, 3, DATE'1997-11-11')
    
    SQL_7
    and p_type = 'STANDARD ANODIZED BRASSâ€™
    
    SQL_12
    and l_shipmode in ('FOB', 'TRUCK') 
    and l_receiptdate >= date '1995-01-01'
    and l_receiptdate < DATEADD(YEAR, 1, date '1995-01-01')
    
    SQL_15
    ERRORâ€”>SQL execution error: Creating view on shared database 'SNOWFLAKE_SAMPLE_DATA' is not allowed.
    
    SQL_19 
    and p_brand = 'Brand#31'
    
    SQL_22
    SUBSTRING(c_phone FROM 1 FOR 2)â€”>SUBSTRING(c_phone,1,2)
    

<aside>
ğŸ’¡

</aside>
